# -*- coding: utf-8 -*-
"""re-train_DCNN_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aSf4Vm8WTtR18CkL4gYu_yrM0qi_P5IC

This colab notebook is intended to take you through the process of re-training DCNN models with new image/label pairs, validating and testing the new models on a given set of testing image/label pairs

### the four sites of interest here are:
> 22-mile (110 train images, 25 test images)

> 30-mile (100 train images, 25 test images)

> 122-mile (62 train images, 25 test images)

> 145-mile (63 train images, 25 test images)

inputs:
> Previously trained model

> image/label pairs from 22-mile, 30-mile, 122-mile, and 145-mile

> 4-site test set (image/label pairs from the sites mentioned above) - 100 images

Methods:
> import scripts and libraries

> import model files from previously trained model

> retrain model with on-the-fly augmentation of new imagery

> save/export models trained new imagery

> load retrained models and test using 4-site test set.

> test using 4-site test set


Outputs:
> newly trained models

> fold results

> ensemble results
"""

# connect to google drive where your image/label pairs and models are stored

import tensorflow
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image, ImageFilter
import random
import cv2
from random import shuffle
from glob import glob
from imageio import imread, imwrite
from sklearn.model_selection import train_test_split,  KFold
from skimage.morphology import binary_erosion, binary_dilation, square
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization
from tensorflow.keras.layers import Concatenate, Conv2DTranspose, Flatten, Activation, Add
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Conv2D, Conv2DTranspose
from tensorflow.keras.constraints import max_norm
from tensorflow.keras import backend as K
from skimage.transform import resize
from statistics import mean
import sklearn.metrics
from imageio import imread, imwrite
import datetime
import csv
import time
import sklearn.metrics
from sklearn.metrics import f1_score
import datetime
import pandas as pd
import albumentations as A

# set the working directory for this notebook, it should be where the Unet_models.py and support_scripts_08042020.py scripts live.
working_dir = 'M://Chapter3//Four_site_segmentation'
os.chdir(working_dir)
# import the support scripts

from Unet_models import *
from support_scripts_08042020 import *

## set  hyperparameters
# batch size - this depends on RAM availability
batch_size = 3
batch_sz = batch_size

# resize input images to:
size = (128,128)

# define the number of epochs
epochs = 75

# choose optimizer
optimizer = 'adam'

# choose loss function
loss = dice_coef_loss
loss_n = 'dice_coef_loss'

# learning rate
max_lr = 1e-4
max_epochs = epochs
min_lr = 1e-6
lr_ratedecay = cosine_ratedecay(epochs,max_lr)


# choose model name
ct = datetime.datetime.now() # current time
date =  str(ct.month) +"_" + str(ct.day) + "_" +str(ct.year)
model_name = 'Res_Unet' + date

"""First I am going to retrain the model trained on 30-mile to include imagery from 22-mile. For my test set I will use the four site model."""

## Training images
# specify the parent dir with all training images and labels
# the parent dir should have two dirs inside: IMAGES and LABELS, they should contain corresponding images (.jpg) and masks (.png)
all_train_dir =  'M://Chapter3//Four_site_segmentation//Four_site_train'
images_dir_train = all_train_dir + os.sep + 'IMAGES'
labels_dir_train = all_train_dir + os.sep + 'LABELS'

# give the training set a descriptive name which will be used in the model naming and the generation of results
train_set = "all_four"

# load train images and train labels
train_images, train_labels = img_lab_to_list_path(images_dir_train, labels_dir_train)

## Test images
# specify the parent directory where the test images are. These are images which have been held out of training and validation

all_test_dir = 'M://Chapter3//Four_site_segmentation//Four_site_test'
images_dir_test = all_test_dir + os.sep + 'IMAGES'
labels_dir_test = all_test_dir + os.sep + 'LABELS'

# give the test set a descriptive name which will be used in model naming and the generation of results
test_set = "Four_site_100"

test_generator = test_dir_to__test_generator(images_dir_test,labels_dir_test,sz)


## models
# load the models trained on 30-mile

## define directory containing pre-trained models. Or build models from scratch using  the following:
## for each fold (n):
# modeln = res_unet(sz,batch_sz)
# modeln.compile(optimizer = optimizer, loss = loss, metrics = [dice_coef ,'acc'])

# specify the directory where pre-trained models are
model_directory = 'M://Chapter3//Four_site_segmentation//Models//Res_Unet//Dice_Adam//4_site_round1'

## use this function from support_scripts_*****.py to load models into a dictionary

model_dict = load_models(model_directory)

print("Models loaded:",model_dict)

## set new model name, for model output

model_nm = "four_site"
model_filepath = model_name + "_"+ model_nm
model_out_dir = 'M://Chapter3//Four_site_segmentation//Models//Res_Unet//Dice_Adam//4_site_round2'
run_name = 'Re_trained_' + model_name + "_train" + model_nm + "_" + optimizer + '_' + loss_n

# make a directory to put results in
if not os.path.exists(run_name):
    os.mkdir(run_name)

     #new_cwd = cwd + os.sep + model_run_dir
    #os.chdir(run_name)

# Load some classes and functions that will be needed for model training, but could not be integrated into support scripts

class PlotLearning(tensorflow.keras.callbacks.Callback):

    def on_train_begin(self, logs={}):
        self.i = 0
        self.x = []
        self.losses = []
        self.val_losses = []
        self.acc = []
        self.val_acc = []
        #self.fig = plt.figure()
        self.logs = []
    def on_epoch_end(self, epoch, logs={}):
        self.logs.append(logs)
        self.x.append(self.i)
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('dice_coef'))
        self.val_acc.append(logs.get('val_dice_coef'))
        self.i += 1
        print('i=',self.i,'loss=',logs.get('loss'),'val_loss=',logs.get('val_loss'),'dice_coef=',logs.get('dice_coef'),'val_dice_coef=',logs.get('val_dice_coef'))


        #choose a random test image and preprocess
        path = np.random.choice(VAL_images)
        raw = Image.open(path) # open image
        raw = np.array(raw.resize(sz[:2]))/255.
        raw = raw[:,:,0:3]
        #predict the mask
        pred = model.predict(np.expand_dims(raw, 0))
        #mask post-processing
        msk  = pred.squeeze()
        msk = np.stack((msk,)*3, axis=-1)
        msk = msk*255
        msk[msk >= 0.5] = 1
        msk[msk < 0.5] = 0

        #show the mask and the segmented image
        combined = np.concatenate([raw, msk, raw* msk], axis = 1)
        plt.axis('off')
        plt.imshow(combined)
        plt.show(block = False)
        plt.close('all')

class LearningRateScheduler(callbacks.Callback):
    def __init__(self,
                 schedule,
                 learning_rate=None,
                 steps_per_epoch=None,
                 verbose=0):
        super(LearningRateScheduler, self).__init__()
        self.learning_rate = learning_rate
        self.schedule = schedule
        self.verbose = verbose
        self.warmup_epochs = 0
        self.warmup_steps = 0
        self.global_batch = 0

    def on_train_batch_begin(self, batch, logs=None):
        self.global_batch += 1
        if self.global_batch < self.warmup_steps:
            if not hasattr(self.model.optimizer, 'lr'):
                raise ValueError('Optimizer must have a "lr" attribute.')
            lr = self.learning_rate * self.global_batch / self.warmup_steps
            backend.set_value(self.model.optimizer.lr, lr)
            if self.verbose > 0:
                print('\nBatch %05d: LearningRateScheduler warming up learning '
                      'rate to %s.' % (self.global_batch, lr))

    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        lr = float(backend.get_value(self.model.optimizer.lr))

        if epoch >= self.warmup_epochs:
            try:  # new API
                lr = self.schedule(epoch - self.warmup_epochs, lr)
            except TypeError:  # old API
                lr = self.schedule(epoch - self.warmup_epochs)
            if not isinstance(lr, (float, np.float32, np.float64)):
                raise ValueError('The output of the "schedule" function '
                                 'should be float.')
            backend.set_value(self.model.optimizer.lr, lr)

            if self.verbose > 0:
                print('\nEpoch %05d: LearningRateScheduler reducing learning '
                      'rate to %s.' % (epoch + 1, lr))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = backend.get_value(self.model.optimizer.lr)

class TimeHistory(tensorflow.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, epoch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, epoch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)

time_callback = TimeHistory()

# build callbacks
def build_callbacks(filepath, lr_ratedecay, lr, steps_per_epoch,time_callback):

    # set checkpoint file
    model_checkpoint = ModelCheckpoint(filepath, monitor='val_loss',
                                   verbose=2, save_best_only=True, mode='min',
                                   save_weights_only = True)

    # learning rate scheduler setting
    learning_rate_scheduler = LearningRateScheduler(lr_ratedecay, lr, steps_per_epoch,
                                                verbose=1)

    callbacks = [model_checkpoint, learning_rate_scheduler, PlotLearning()]

    return callbacks

filepaths_list = []
kf = KFold(5, shuffle = True)
fold = 0
exts = ['.h5', '.H5']
filelist = sorted(os.listdir(model_directory))
model_str_list = [f for f in filelist if f.endswith(tuple(exts))]
trained_model_dict = {}

for train, val in kf.split(train_images):
    fold_start_time = datetime.datetime.now()
    fold +=1
    mod = 'model_' + str(fold)
    model = model_dict[mod]
    model_name = os.path.splitext(model_str_list[fold - 1])[0]
    nfilepath = model_filepath + str(fold) + ".h5"
    print(nfilepath, "-nfilepath")
    filepaths_list.append(nfilepath)
    print(f"Fold #{fold}")
    TRAIN_images = train_images[train]
    TRAIN_labels = train_labels[train]
    VAL_images = train_images[val]
    VAL_labels = train_labels[val]
    train_generator = image_generator_augment(TRAIN_images,TRAIN_labels, batch_size=batch_size, sz = sz)
    val_generator = image_generator_augment(VAL_images,VAL_labels, batch_size=batch_size, sz = sz)
    train_steps = (len(TRAIN_images) //batch_size)*2 # note // is floor division, and thus provides no remainder
    val_steps = len(VAL_images) //batch_size
    steps_per_epoch = (len(TRAIN_images) // batch_size)*2
    history = model.fit(train_generator,
                    epochs = max_epochs, steps_per_epoch = train_steps,
                    validation_data = val_generator, validation_steps = val_steps,
                    callbacks = build_callbacks(model_filepath,lr_ratedecay, max_lr, steps_per_epoch, time_callback))
    fold_end_time = datetime.datetime.now()
    name = run_name + os.sep + 'results_' + model_name + '_fold' + str(fold)
    try:
      plot_history_all(history, save = True, fig_run_name = name)
      model.save_weights(working_dir + os.sep + run_name + os.sep + nfilepath)
      model.save_weights(model_out_dir + os.sep + nfilepath)
    except IOError:
      print('problem with plot_all_history')
      model.save_weights(working_dir + os.sep + run_name + os.sep + nfilepath)
      model.save_weights(model_out_dir + os.sep + nfilepath)
    else:
      print('problem with plot_all_history')
      model.save_weights(working_dir + os.sep + run_name + os.sep + nfilepath)
      model.save_weights(model_out_dir + os.sep + nfilepath)

# test models

# import and load the retrained models
trained_model_dict = load_models(model_out_dir)
print(trained_model_dict)

# specify the folder where the results dataframe will go
results_folder = run_name

# set the weights for the ensemble
weights = (1,1,1,1,1)

X, Y = next(test_generator)
Y_preds1 = []
Y_preds2 = []
Y_preds3 = []
Y_preds4 = []
Y_preds5 = []
Y_preds_ensemble = []
thresh = 0.5

# !!!!!figure out how to interrofate the trained_model_dict

for i in range(0, len(X)):
    model1 = trained_model_dict['model_1']
    pred1 = model1.predict(np.expand_dims(X[i],0))
    msk1 = pred1.squeeze()
    msk1 = np.stack((msk1,)*3, axis=-1)
    msk1[msk1 >= thresh] = 1
    msk1[msk1 < thresh] = 0
    Y_preds1.append(msk1[:,:,0])

    model2 = trained_model_dict['model_2']
    pred2 = model2.predict(np.expand_dims(X[i],0))
    msk2 = pred2.squeeze()
    msk2 = np.stack((msk2,)*3, axis=-1)
    msk2[msk2 >= thresh] = 1
    msk2[msk2 < thresh] = 0
    Y_preds2.append(msk2[:,:,0])

    model3 = trained_model_dict['model_3']
    pred3 = model3.predict(np.expand_dims(X[i],0))
    msk3 = pred3.squeeze()
    msk3 = np.stack((msk3,)*3, axis=-1)
    msk3[msk3 >= thresh] = 1
    msk3[msk3 < thresh] = 0
    Y_preds3.append(msk3[:,:,0])

    model4 = trained_model_dict['model_4']
    pred4 = model4.predict(np.expand_dims(X[i],0))
    msk4 = pred4.squeeze()
    msk4 = np.stack((msk4,)*3, axis=-1)
    msk4[msk4 >= thresh] = 1
    msk4[msk4 < thresh] = 0
    Y_preds4.append(msk4[:,:,0])

    model5 = trained_model_dict['model_5']
    pred5 = model5.predict(np.expand_dims(X[i],0))
    msk5 = pred5.squeeze()
    msk5 = np.stack((msk5,)*3, axis=-1)
    msk5[msk5 >= thresh] = 1
    msk5[msk5 < thresh] = 0
    Y_preds5.append(msk5[:,:,0])

    merged = np.dstack((np.squeeze(pred1), np.squeeze(pred2), np.squeeze(pred3), np.squeeze(pred4), np.squeeze(pred5)))
    pred = np.average(merged, axis=2, weights=weights) ###
    msk = pred.squeeze()
    msk = np.stack((msk,)*3, axis=-1)
    msk[msk >= thresh] = 1
    msk[msk < thresh] = 0
    Y_preds_ensemble.append(msk[:,:,0])


f1_score_fold1 = []
f1_score_fold2 = []
f1_score_fold3 = []
f1_score_fold4 = []
f1_score_fold5 = []
f1_score_ensemble = []

for i in range(0, len(Y)):
    y_true = Y[i].squeeze()
    # now for the predicted Y
    y_pred1 = Y_preds1[i]
    f1_micro1 = f1_score(y_true, y_pred1, average='micro')
    f1_score_fold1.append(f1_micro1)

    y_pred2 = Y_preds2[i]
    f1_micro2 = f1_score(y_true, y_pred2, average='micro')
    f1_score_fold2.append(f1_micro2)

    y_pred3 = Y_preds3[i]
    f1_micro3 = f1_score(y_true, y_pred3, average='micro')
    f1_score_fold3.append(f1_micro3)

    y_pred4 = Y_preds4[i]
    f1_micro4 = f1_score(y_true, y_pred4, average='micro')
    f1_score_fold4.append(f1_micro4)

    y_pred5 = Y_preds5[i]
    f1_micro5 = f1_score(y_true, y_pred5, average='micro')
    f1_score_fold5.append(f1_micro5)

    y_pred_ensemble = Y_preds_ensemble[i]
    f1_micro_en = f1_score(y_true, y_pred_ensemble, average='micro')
    f1_score_ensemble.append(f1_micro_en)

results = []

f1_fold1 = np.mean(f1_score_fold1)
print(f"Mean F1 score fold1 {f1_fold1}")
run1 = {'Train_set':train_set, 'Fold':"one", "Test_set":test_set, 'Mean f1 micro': f1_fold1}
results.append(run1)

f1_fold2 = np.mean(f1_score_fold2)
print(f"Mean F1 score fold2 {f1_fold2}")
run2 = {'Train_set':train_set, 'Fold':"two", "Test_set":test_set, 'Mean f1 micro': f1_fold2}
results.append(run2)

f1_fold3 = np.mean(f1_score_fold3)
print(f"Mean F1 score fold3 {f1_fold3}")
run3 = {'Train_set':train_set, 'Fold':"three", "Test_set":test_set, 'Mean f1 micro': f1_fold3}
results.append(run3)

f1_fold4 = np.mean(f1_score_fold4)
print(f"Mean F1 score fold4 {f1_fold4}")
run4 = {'Train_set':train_set, 'Fold':"four", "Test_set":test_set, 'Mean f1 micro': f1_fold4}
results.append(run4)

f1_fold5 = np.mean(f1_score_fold5)
print(f"Mean F1 score fold5 {f1_fold5}")
run5 = {'Train_set':train_set, 'Fold':"five", "Test_set":test_set, 'Mean f1 micro': f1_fold5}
results.append(run5)

all = [f1_fold1,f1_fold2,f1_fold3,f1_fold4,f1_fold5]
run_mean = {'Train_set':train_set, 'Fold':"mean", "Test_set":test_set, 'Mean f1 micro': np.mean(all)}

print(f"Mean F1 score ensemble {np.mean(f1_score_ensemble)}")
rune = {'Train_set':train_set, 'Fold':"ensemble", "Test_set":test_set, 'Mean f1 micro': np.mean(f1_score_ensemble)}
results.append(rune)

model_used = {'Train_set':train_set, 'Fold':"all", "Test_set":test_set, 'Mean f1 micro': "NA", "Model": model_filepath, "Model_dir": model_out_dir }
results.append(model_used)

df = pd.DataFrame(results)
print(df)
print(os.getcwd())


results_filename = run_name + os.sep + "results_test_" + train_set +'_train_'+ test_set +'.csv'
df.to_csv(results_filename)
